304*35+1827
328*35+985
localFilepath <-"C:/Users/Oâ€™ReganPaul/Downloads"
read.csv(file = paste(localFilepath,"REACT_pharmacy_15Jan21.csv", sep = "/"), stringsAsFactors = FALSE, header = TRUE)
pharmacy <- read.csv(file = paste(localFilepath,"REACT_PharmacData_15Jan21.csv", sep = "/"), stringsAsFactors = FALSE, header = TRUE)
pharmacy <- read.csv(file = paste(localFilepath,"REACT_PharmacyData_15Jan21.csv", sep = "/"), stringsAsFactors = FALSE, header = TRUE)
unique(pharmacy$DRUGDESCRIPTION)
grep(pattern = "DEXAM", x=pharmacy$DRUGDESCRIPTION)
unique(pharmacy$STUDY_ID[grep(pattern = "DEXAM", x=pharmacy$DRUGDESCRIPTION)])
length(unique(pharmacy$STUDY_ID[grep(pattern = "DEXAM", x=pharmacy$DRUGDESCRIPTION)]))
rm(list=ls())
library(ISLR)
library(caret)
data(Wage)
head(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE) # partition into 70% trainind set
training <- Wage[inTrain, ]  ## create training set
testing <- Wage[-inTrain, ]  ## create test set
unique(training$jobclass)
dummies <- dummyVars(wage ~ jobclass, data = training) ## create dummy variables, 0 and 1 for each level
head(dummies)
head(predict(dummies, newdata = training))
head(predict(dummies, newdata = training)) ## assign dummy variable values to each sample in the training set
nsv <- nearZeroVar(training, saveMetrics = TRUE) ## identify which variables in the training set have very little variability
nsv
library(splines)
bsBasis <- bs(training$age, df=3)
bsBasis
lml <- lm)wage ~ bsBasis, data=training
lml <- lm(wage ~ bsBasis, data=training)
plot(training$age, training$wage,pch=19,cex=0.5)
points(training$age,predict(lml,newdata = training), col="red",pch=19,cex=0.5)
predict(bsBasis, age=testing$age)
plot(training$age, training$wage,pch=19,cex=0.5)
predict(bsBasis, age=testing$age)
rm(list=ls())
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[, -58])) ## column 58 is the outcome of interest
M
diag(M)
diag(M) <- 0
M
which(M > 0.8, arr.ind = TRUE)
names(spam)[c(34,32)]
plot(spam[,34], spam[,32])
names(spam)[c(34,32,40)]
X <- training$num415 + training$num857
Y <- training$num415 - training$num857
plot(X,Y)
smallSpam <- spam[ ,c(34,32)]   ## just get the num415 and num857 columns for simplicity
prComp <- prcomp(smallSpam)  ## get principal components
prComp
plot(prComp$x[,1], prComp$x[,2])
prComp$x
tail(prComp$x)
unique(prComp$x$PC1)
unique(prComp$x[,1])
summary(prComp)
str(prComp)
class(prComp$x)
prComp$rotation
preProc <- preProcess(log10(spam[,-58]+1), method = "pca", pcaComp = 2)
summary(preProc)
head(preProc)
spamPC <- predict(preProc, log10(spam[ ,-58]+1)) ##
plot(spamPC[,1], spamPC[,2], col=typeColor)
typeColor <- ((spam$type=="spam")*1 +1)
typeColor
unique(typeColor)
prComp <- prcomp(log10(spam[,-58]+1)) ## take log10 of variables, add 1 and calculate principal components
plot(prComp$x[,1], prComp$x[, 2], col=typeColor, xlab="PC1", ylab="PC2")
> spamPC <- predict(preProc, log10(spam[ ,-58]+1)) ##
> plot(spamPC[,1], spamPC[,2], col=typeColor)
spamPC <- predict(preProc, log10(spam[ ,-58]+1)) ##
plot(spamPC[,1], spamPC[,2], col=typeColor)
prComp$rotation
prComp$rotation[ ,1]
preProc <- preProcess(log10(spam[,-58]+1), method = "pca", pcaComp = 2)
class(preProc)
preProc <- preProcess(log10(training[,-58]+1), method = "pca", pcaComp = 2)
trainPC <- predict(preProc, log10(training[, -58]+1))
modelFit <- train(training$type ~ ., method="glm", data=trainPC) ##
inTrain
training <- spam[inTrain,]
testing <- spam[-inTrain, ]
preProc <- preProcess(log10(training[,-58]+1), method = "pca", pcaComp = 2)
trainPC <- predict(log10(training[,-58]+1))
trainPC <- predict(preProc, log10(training[,-58]+1))
modelFit <- train(training$type ~ ., method="glm", data = trainPC)
rm(list=ls())
library(caret)
data("faithful")
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue",xlab = "Waiting time", ylab = "Eruption duration")
lm1 <- lm(eruptions ~ waiting, data=trainFaith)
summary(lm1)
plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue",xlab = "Waiting time", ylab = "Eruption duration")
lines(trainFaith$waiting, lm1$fitted.values, lwd=3)
newdata <- data.frame(waiting=80)
predict(lm1, newdata)
plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue",xlab = "Waiting time", ylab = "Eruption duration")
lines(trainFaith$waiting, lm1$fitted.values, lwd=3)
sqrt(sum((predict(lm1,newdata = testFaith)-testFaith$eruptions)^2))
predict(lm1,newdata = testFaith)-testFaith$eruptions
(predict(lm1,newdata = testFaith)-testFaith$eruptions)^2
mean((predict(lm1,newdata = testFaith)-testFaith$eruptions)^2)
sqrt(mean((predict(lm1,newdata = testFaith)-testFaith$eruptions)^2)
)
sqrt(mean((predict(lm1,newdata = testFaith)-testFaith$eruptions)^2))
sqrt(mean((predict(lm1,newdata = trainFaith)-trainFaith$eruptions)^2))
pred1 <- predict(lm1, newdata = testFaith, interval = "prediction")
class(pred1)
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue") ## plot actual values in test set
matlines(testFaith$waiting[ord], pred1[ord], type = "l", col = c(1,2,2), lty = c(1,1,1), lwd = 3) ## add lines with predicted values and prediction intervals
matlines(testFaith$waiting[ord], pred1[ord, ], type = "l", col = c(1,2,2), lty = c(1,1,1), lwd = 3) ## add lines with predicted values and prediction intervals
ord
pred1
pred1[ord, ] ## reorder on waiting time
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue") ## plot actual values in test set
matlines(testFaith$waiting, pred1 , type = "l", col = c(1,2,2), lty = c(1,1,1), lwd = 3) ## add lines with predicted values and prediction intervals
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue") ## plot actual values in test set
matlines(testFaith$waiting[ord], pred1[ord, ], type = "l", col = c(1,2,2), lty = c(1,1,1), lwd = 3) ## add lines with predicted values and prediction intervals
modFit <- train(eruptions ~ waiting, data=trainFaith, method="lm")
summary(modFit$finalModel)
predict(modFit, newdata)
pred2 <- predict(modFit, newdata = testFaith, interval="prediction")
head(pred2)
predict(modFit, newdata = testFaith, interval="prediction")
predict(modFit$finalModel, newdata = testFaith, interval="prediction")
pred2 <- predict(modFit$finalModel, newdata = testFaith, interval="prediction")
head(pred2)
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue") ## plot actual values in test set
matlines(testFaith$waiting, pred2, type = "l", col = c(1,2,2), lty = c(1,1,1), lwd = 3)
?train
library(ISLR)
data("Wage")
Wage <- subset(Wage, select = -c(logwage))
inTrain <- createDataPartition(y-Wage$wage, p=0.7, list = FALSE)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
qplot(age, wage, colour=jobclass, data=training)  ## exploratory plot
qplot(age, wage, colour=education, data=training)  ## exploratory plot
modeFit <- train(wage ~ age + jobclass + education,
method = "lm",
data=training)
finMod <- modFit$finalModel
print(modFit)
modFit <- train(wage ~ age + jobclass + education,
method = "lm",
data=training)
finMod <- modFit$finalModel
print(modFit)
plot(finMod, pch=19, cex=0.5,col="#00000010")
qplot(finMod$fitted.values, finMod$residuals, colour=race, data=training)
plot(finMod$residuals, pch=19)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisaease)
data(AlzheimerDisease)
data("concrete")
set.seed(1000)
inTrain <- createDataPartition(mixtures$CompressiveStrength, p=3/4)[[1]]
training <- mixtures[inTrain, ]
testing <- mixtures[-inTrain, ]
qplot(training$CompressiveStrength)
plot(training$CompressiveStrength)
names(training)
plot(training$CompressiveStrength, colour="Cement")
qplot(training$CompressiveStrength, colour="Cement")
qplot(training$CompressiveStrength,seq_along(training$CompressiveStrength) colour="Cement")
qplot(training$CompressiveStrength,seq_along(training$CompressiveStrength), colour="Cement")
plot(training$CompressiveStrength, colour="Cement")
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour="Cement")
names(training)
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour="BlastFurnaceSlag")
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour="FlyAsh")
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=Cement)
unique(training$Cement)
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour="Cement")
?cut2
libary(Hmisc)
library(Hmisc)
?cut2
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=cut2(training$Cement))
head(training)
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=cut2(training$BlastFurnaceSlag))
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=cut2(training$FlyAsh))
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=cut2(training$Water))
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=cut2(training$Age))
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength)
qplot(seq_along(training$CompressiveStrength),training$CompressiveStrength, colour=cut2(training$FlyAsh))
qplot(training$Superplasticizer)
log(training$Superplasticizer)
set.seed(3433)
adData <- data.frame(diagnosis, predictors)
inTrain <- createDataPartition(adData$diagnosis, p=3/4)[[1]]
training <- adData[inTrain, ]
testing <- adData[-inTrain, ]
names(training)
grep(pattern = "IL", x=names(training))
head(training[ ,grep(pattern = "IL", x=names(training))])
head(training[ ,grep(pattern = "^IL", x=names(training))])
subTraining <- training[ ,grep(pattern = "^IL", x=names(training))]
prComp <- prcomp(subTraining)
head(prComp)
summary(prComp)
preObj <- preProcess(training[ ,grep(pattern = "^IL", x=names(training))])
summary(preObj)
class(preObj)
preObj$rotation
set.seed(3433)
data(AlzheimerDisease)
names(training)
rm(list=ls())
rm(list=ls())
data("iris")
library(caret)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
modFit <- train(Species ~ ., data=training, method="rf", prox=TRUE)
modFit <- train(Species ~ ., data=training, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel, k=2)
library(randomForest)
getTree(modFit$finalModel, k=2)
irisP <- classCenter(training[ ,c(3,4)], training$Species, modFit$finalModel$proximity)
class(irisP)
irisP <- as.data.frame(irisP)
head(irisP)
irisP
irisP$Species <- rownames(irisP)
irisP
p <- qplot(Petal.Width, Petal.Length, col=Species, data=training)
p+ geom_point(aes(x=Petal.Width, y=Petal.Length, col=Species), size=5, shape=4, data=irisP)
pred <- predict(modFit, testing)
testing$predRight <- pred==testing$Species
table(pred, testing$Species)
qplot(Petal.Width, Petal.Length, colour=predRight,data=testing, main="newdata Predictions")
rm(list=ls())
library(caret)
data("iris")
head(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain, ]
testing <- iris[ -inTrain, ]
head(training)
sample(x=training, size=5, replace = TRUE)
?sample
X[sample(nrow(training),size=5,replace=TRUE),]
training[sample(nrow(training),size=5,replace=TRUE),]
training[sample(nrow(training),size=5,replace=TRUE),]
training[sample(nrow(training),size=5,replace=TRUE),]
training[sample(nrow(training),size=5,replace=TRUE),]  ## sample 5 rows from training dataset, all variables
training[sample(nrow(training),size=5,replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## sample 5 rows and 2 variables from training dataset
training[sample(nrow(training),size=5,replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## sample 5 rows and 2 variables from training dataset
training[sample(nrow(training),size=5,replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## sample 5 rows and 2 variables from training dataset
split0 <- training[sample(nrow(training),size=5,replace=TRUE),]  ## sample 5 rows from training dataset, all variables
split1 <- training[sample(nrow(training),size=5,replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## sample 5 rows and 2 variables from training dataset
split2 <- training[sample(nrow(training),size=5,replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## sample 5 rows and 2 variables from training dataset
install.packages("ineq")
library(ineq)
modFit <- train(Species ~ ., method="rpart", data=split0)
head(split0)
split0 <- training[sample(nrow(training),size=nrow(training),replace=TRUE),]  ## sample 5 rows from training dataset, all variables
modFit <- train(Species ~ ., method="rpart", data=split0)
library(rattle)
fancyRpartPlot(modFit$finalModel)
split(split0, Petal.Length < 2.6)
split(split0, split0$Petal.Length < 2.6)
split(split0, split0$Petal.Length < 2.6)[[1]]
split1.1 <- split(split0, split0$Petal.Length < 2.6)[[1]]
head(split1.1)
split1.2 <- split(split0, split0$Petal.Length < 2.6)[[2]]
head(split1.2)
split1.1 <- training[sample(nrow(training),size=nrow(split1.1),replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## resample all rows and 2 variables from the result of the first split
head(split1.1)
modFit <- train(Species ~ ., method="rpart", data=split1.1)
fancyRpartPlot(modFit$finalModel)
split2.1 <- split(split1.1, split1.1$Sepal.Length < 5.5)[[1]]
head(split2.1)
split2.2 <- split(split1.1, split1.1$Sepal.Length < 5.5)[[2]]
head(split2.2)
bootstrap1 <- training[sample(nrow(training),size=nrow(training),replace=TRUE),]  ## sample rows from training dataset, all variables
split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[1]]
split1.1 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[1]]
head(split1.1)
split1.2 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[2]]
split1.1 <- training[sample(nrow(training),size=nrow(split1.1),replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## resample all rows and 2 variables from the result of the first split
head(split1.1)
split1.2 <- training[sample(nrow(training),size=nrow(split1.2),replace=TRUE),c(sample(1:4,size=2,replace=TRUE),5)]  ## resample all rows and 2 variables from the result of the first split
head(split1.2)
available.for.split <- sample(names(split1.1)[1:4], size = 4, replace = TRUE))
sample(names(split1.1)[1:4], size = 4, replace = TRUE)
sample(names(split1.1)[1:4], size = 4, replace = TRUE)
sample(names(split1.1)[1:4], size = 4, replace = TRUE)
> split1.1 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[1]]
> split1.2 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[2]]
split1.1 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[1]]
split1.2 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[2]]
sample(names(split1.1)[1:4], size = 4, replace = TRUE)
sample(names(split1.1)[1:4], size = 4, replace = TRUE)
sample(names(split1.1)[1:4], size = 4, replace = TRUE)
available.for.split <- sample(names(split1.1)[1:4], size = 4, replace = TRUE)
available.for.split
head(split1.1)
head(split1.2)
?getTree
library(randomForest)
?getTree
modFit <- train(Species ~ ., data = training, method="rf", prox=TRUE)
getTree(modFit$finalModel, k=1)
class(getTree(modFit$finalModel, k=1))
fancyRpartPlot(getTree(modFit$finalModel, k=1))
cforest(Species ~ ., data=training, controls=cforest_control(mtry=2, mincriterion=0))
rm(list=ls())
data("iris")
library(caret)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
head(training)
> modFit <- train(Species ~ ., method="rpart", data=training) ## rpart is R's method for creating decision trees
modFit <- train(Species ~ ., method="rpart", data=training) ## rpart is R's method for creating decision trees
library(rattle)
fancyRpartPlot(modFit$finalModel) ## nicer looking plot
split1.1 <- split(training, Petal.Length<2.5)[[1]]
split1.1 <- split(training, training$Petal.Length<2.5)[[1]]
split1.2 <- split(training, training$Petal.Length<2.5)[[2]]
head(split1.1,10)
head(split1.2,10)
head(training,10)
table(iris$Species)
table(training$Species)
table(split1.1$Species)
table(split1.2$Species)
split2.1 <- split(split1.1, split1.1$Petal.Width<1.6)[[1]]
split2.2 <- split(split1.1, split1.1$Petal.Width<1.6)[[2]]
head(split2.1)
table(split2.1$Species)
head(split2.2)
table(split2.2$Species)
bootstrap1 <- training[sample(nrow(training),size=nrow(training),replace=TRUE),]
split1.1 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[1]]
table(split1.1$Species)
split1.2 <- split(bootstrap1, bootstrap1$Petal.Length < 2.6)[[2]]
table(split1.2$Species)
available.for.split <- sample(names(split1.1)[1:4], size = 4, replace = TRUE)
available.for.split
available.for.split <- sample(names(split1.1)[1:4], size = 4, replace = TRUE)
available.for.split
available.for.split <- sample(names(split1.1)[1:4], size = 4, replace = TRUE)
available.for.split
available.for.split <- sample(names(split1.1)[1:4], size = 4, replace = TRUE)
available.for.split
read.csv(file = "https://civicdb.org/downloads/nightly/nightly-ClinicalEvidenceSummaries.tsv", sep = "\t")
evidence <- read.csv(file = "https://civicdb.org/downloads/nightly/nightly-ClinicalEvidenceSummaries.tsv", sep = "\t")
dim(evidence)
names(evidence)
"H773_V774insAH" %in% evidence$variant
filter(evidence, gene=="EGFR")
library(dplyr)
filter(evidence, gene=="EGFR")
unique(filter(evidence, gene=="EGFR"))
unique(filter(evidence, gene=="EGFR"))$variant
sort(unique(filter(evidence, gene=="EGFR"))$variant)
sort(unique(filter(evidence, gene=="MYC"))$variant)
sort(unique(filter(evidence, gene=="MYC")))
unique(filter(evidence, gene=="MYC"))
sort(unique(filter(evidence, gene=="FGFR3"))$variant)
unique(filter(evidence, gene=="FGFR3"))
unique(filter(evidence, gene=="KRAS"))
unique(filter(evidence, gene=="KRAS"))$variant
sort(unique(filter(evidence, gene=="FGFR3")$variant))
sort(unique(filter(evidence, gene=="KRAS")$variant))
unique(filter(evidence, gene=="KRAS" & variant =="Q61R"))
unique(filter(evidence, gene=="KRAS" & variant =="Q61*"))
unique(filter(evidence, gene=="KRAS" & variant =="Q61.*"))
unique(filter(evidence, gene=="KRAS" & variant =="Q61*"))
unique(filter(evidence, gene=="KRAS" & variant =="Q61"))
rm(list=ls())
getwd()
Sys.info()
packageVersion("tidyr")
packageVersion("dplyr")
installed. packages()
installed.packages()
class(installed.packages())
dim(installed.packages())
names(installed.packages())
as.data.frame(installed.packages())
write.csv(as.data.frame(installed.packages()), file = "systemInfo.csv")
getwd()
setwd("GitHub/cancer-trial-match")
rm(list=ls())
rmarkdown::render("trialMatchDataRefresh.Rmd")
tic("run trial match data refresh")
rmarkdown::render("trialMatchDataRefresh.Rmd")
toc()
dbDisconnect(conn2)
rm(list=ls())
getwd()
rmarkdown::render("trialMatchDataRefresh.Rmd")
# Chunk 1: setup
require(flexdashboard)
require(dplyr)
require(tidyr)
require(kableExtra)
require(shinyWidgets)
require(leaflet)
require(htmltools)
require(rjson)
require(RSQLite)
#require(DT)
## load configuration data from JSON file
configuration <- fromJSON(file = "trialMatchConfiguration.json")
# Chunk 2: connect to SQLite DB
# Create an RSQLite database
con <- dbConnect(RSQLite::SQLite(), "trialMatchData.sqlite")
# Chunk 3: read cancer study data into memory
## read cancerStudies table into memory
cancerStudies <- dbGetQuery(con, "SELECT * FROM cancerStudies")
## contains study details, mapped to a controlled set of cancer types in the "TARGET.condition"  column
## names are
# [1] "interventions"      "locations"          "postcode"
#  [4] "nct_id"             "brief_title"        "overall_status"
#  [7] "condition"          "site_name"          "site_status"
# [10] "investigators"      "contacts"           "Refresh.date"
# [13] "matching.condition" "TARGET.condition"   "Link"
# [16] "postcode.lat"       "postcode.long"      "lat"
# [19] "long"               "ParentTerm"
# no columns are aggregated
## filter and retain only studies with overall status of "Recruiting"
cancerStudies <- dplyr::filter(cancerStudies, overall_status == "Recruiting")
## drop postcode.lat and postcode.long columns
cancerStudies <- unique(dplyr::select(cancerStudies, -c("postcode.lat", "postcode.long")))
## rename parentTerm column as "Mechanism"
cancerStudies <- rename(cancerStudies, "Mechanism"="ParentTerm")
## read conditions and synonyms into memory
# conditionSynonyms <- read.csv(file="conditionSynonyms4.csv", header = TRUE, stringsAsFactors = FALSE)
# Chunk 4: get a full set of all study locations
cancer.study.locations <- unique(dplyr::select(cancerStudies, nct_id, lat, long))
## drop any rows with missing lat/long values
## NOTE THAT THIS MEANS THESE STUDIES WILL NOT BE DISPLAYED ON MAP
## (BUT WILL BE IN THE TABLE UNDERNEATH)
cancer.study.locations <- cancer.study.locations[complete.cases(cancer.study.locations), ]
# Chunk 5: get mechanisms
## not needed, as mechanisms are already included in cancerStudies$ParentTerm
## read drugs.targets table into memory
# targetMatches <- dbGetQuery(con, "SELECT * FROM drugsTargets")
#
#
#
# targetMatches <- unique(read.table(file = "targetMatches.tsv", header = TRUE, stringsAsFactors = FALSE))
# ## drop unneeded columns
# targetMatches <- unique(dplyr::select(targetMatches, Interventions, Mechanism))
#
# ## join to cancerStudies table
# cancerStudies <- merge(x=cancerStudies, by.x="interventions", all.x=TRUE, y=targetMatches, by.y="Interventions")
# Chunk 6: aggregate cancerStudies into compact form
## aggregating interventions was causing some study:intervention combinations to be missed out
cancerStudies <- cancerStudies %>%
group_by_at(vars(-c( locations, postcode, condition, site_name, site_status, investigators, contacts, lat, long, Refresh.date))) %>%
summarize(locations = toString(sort(unique(na.omit(locations)))),
sites = toString(sort(unique(na.omit(site_name)))),
conditions = toString(sort(unique(na.omit(condition)))),
investigators = toString(sort(unique(na.omit(investigators)))),
contacts = toString(sort(unique(na.omit(contacts))))) %>%
as.data.frame()
# Chunk 7: get prior tx exclusions
## read indexed eligibilities table into memory
indexedEligibility <- dbGetQuery(con, "SELECT * FROM indexedEligibility")
## filter for exclusions
excludedTX <- filter(indexedEligibility, criterion.type=="EXCLUSION")
## filter for prior therapy
excludedTX <- filter(excludedTX, feature=="PRIOR_THERAPY")
## drop unnecessary columns
excludedTX <- unique(dplyr::select(excludedTX, nct_id, "Exclusions"="criteria"))
## aggregate into single row per study
excludedTX <- excludedTX %>%
group_by(nct_id) %>%
summarize(Exclusions = paste(sort(unique(na.omit(Exclusions))), collapse="\n")) %>%
as.data.frame()
## join to cancerStudies
cancerStudies <- merge(x=cancerStudies, by.x = "nct_id", all.x = TRUE, y=excludedTX, by.y="nct_id")
# Chunk 8: read studies scored on match against genes of interest
## read cancerStudies table into memory
scoredMatches <- dbGetQuery(con, "SELECT * FROM scoredMatches")
## scoredMatches contains all possible matches of studies, their interventions and inclusion criteria against all possible genes of interest
## column names are
# [1] "symbol"                 "variant_type"           "nct_id"
# [4] "intervention_rationale" "eligibility_rationale"  "matching_criteria"
# [7] "combined_score"
## conatenate symbol and variant_type columns to get <gene name> <variant_type>, e.g. "EGFR mutation"
#scoredMatches$gene_variant_type <- paste(scoredMatches$symbol, scoredMatches$variant_type, sep=" ")
# scoredMatches <- unique(read.table(file = "scoredMatches.tsv", header = TRUE, stringsAsFactors = FALSE))
## column names are "nct_id, gene.of.interest, intervention.name, intervention.target, rationale, confidence, criteria"
View(scoredMatches)
scoredMatches[order(scoredMatches$combined_score, decreasing = TRUE), ]
View(scoredMatches)
